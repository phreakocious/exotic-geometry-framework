#!/usr/bin/env python3
"""
Reaction-Diffusion Patterns Investigation: Turing Instabilities
===============================================================

Can exotic geometries distinguish between different Turing patterns
(spots, stripes, labyrinths, chaos) generated by the Gray-Scott model?

The Gray-Scott reaction-diffusion system produces a wide variety of 
morphologies from just two parameters (feed rate F, kill rate k).
We investigate if these morphologies have distinct geometric signatures.

DIRECTIONS:
D1: Taxonomy — Pairwise distinguishability of 6 regimes (spots, stripes, etc.)
D2: Sequential structure — All regimes vs spatially shuffled (structure destruction)
D3: Parameter sweep — Transition from spots to stripes (varying F)
D4: Robustness — Signal degradation with added observation noise
D5: Scale test — Geometric stability across grid resolutions
"""

import sys
import numpy as np
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
from tools.investigation_runner import Runner

# ==============================================================
# CONFIG
# ==============================================================
# We use 64x64 for speed in automated testing, but 128x128 is better for detail.
# Runner defaults will be used, but we can override data_size in main.
N_STEPS = 2000  # Sufficient for pattern formation
Du, Dv = 1.0, 0.5

# Gray-Scott Regimes (Pearson/Sims)
REGIMES = {
    'spots':   (0.034, 0.063),   # η
    'stripes': (0.042, 0.059),   # θ
    'worms':   (0.054, 0.063),
    'coral':   (0.026, 0.061),   # λ
    'mazes':   (0.029, 0.057),
    'chaos':   (0.014, 0.054),   # β
}

# ==============================================================
# DATA GENERATORS
# ==============================================================

def laplacian_9pt(field):
    """9-point discrete Laplacian with periodic BCs."""
    p = np.pad(field, 1, mode='wrap')
    H, W = field.shape
    return (0.2 * (p[0:H, 1:W+1] + p[2:H+2, 1:W+1] +
                   p[1:H+1, 0:W] + p[1:H+1, 2:W+2]) +
            0.05 * (p[0:H, 0:W] + p[0:H, 2:W+2] +
                    p[2:H+2, 0:W] + p[2:H+2, 2:W+2]) - field)

def generate_gray_scott(rng, size, F=0.054, k=0.062, noise_level=0.0):
    """
    Generate Gray-Scott pattern.
    Returns V concentration (activator) as float64 2D array.
    """
    H, W = size, size
    u = np.ones((H, W))
    v = np.zeros((H, W))

    # Initial perturbation: central square + random noise
    cx, cy = W // 2, H // 2
    sz = max(4, min(H, W) // 8)
    
    # Add randomness to initial condition for independence
    u[cy-sz:cy+sz, cx-sz:cx+sz] = 0.50 + 0.05 * rng.standard_normal((2*sz, 2*sz))
    v[cy-sz:cy+sz, cx-sz:cx+sz] = 0.25 + 0.05 * rng.standard_normal((2*sz, 2*sz))
    
    # Random spots to break symmetry further
    for _ in range(rng.integers(3, 8)):
        ry, rx = rng.integers(0, H), rng.integers(0, W)
        rs = rng.integers(2, 6)
        y0, y1 = max(0, ry-rs), min(H, ry+rs)
        x0, x1 = max(0, rx-rs), min(W, rx+rs)
        v[y0:y1, x0:x1] += 0.2

    # Simulation loop
    # We can use a simpler 5-point laplacian for speed or the 9-point one.
    # 9-point is more isotropic.
    
    for _ in range(N_STEPS):
        Lu = laplacian_9pt(u)
        Lv = laplacian_9pt(v)
        uvv = u * v * v
        u += Du * Lu - uvv + F * (1.0 - u)
        v += Dv * Lv + uvv - (F + k) * v
        np.clip(u, 0, 1, out=u)
        np.clip(v, 0, 1, out=v)

    if noise_level > 0:
        v += rng.normal(0, noise_level, v.shape)
        np.clip(v, 0, 1, out=v)

    return v.astype(np.float64)

# ==============================================================
# DIRECTIONS
# ==============================================================

def direction_1(runner):
    """D1: Taxonomy — Pairwise distinguishability of 6 regimes."""
    print("\n" + "=" * 60)
    print("D1: TAXONOMY")
    print("=" * 60)

    conditions = {}
    for name, (F, k) in REGIMES.items():
        with runner.timed(name):
            chunks = [generate_gray_scott(rng, runner.data_size, F=F, k=k)
                      for rng in runner.trial_rngs()]
            conditions[name] = runner.collect(chunks)

    matrix, names, _ = runner.compare_pairwise(conditions)
    return dict(matrix=matrix, names=names)

def direction_2(runner):
    """D2: Sequential structure — All regimes vs shuffled."""
    print("\n" + "=" * 60)
    print("D2: SEQUENTIAL STRUCTURE (vs Shuffled)")
    print("=" * 60)

    results = {}
    # Test a subset to save time/space if needed, or all. Let's do 3 distinct ones.
    targets = ['spots', 'mazes', 'chaos'] 
    
    for name in targets:
        F, k = REGIMES[name]
        chunks = [generate_gray_scott(rng, runner.data_size, F=F, k=k)
                  for rng in runner.trial_rngs()]
        real = runner.collect(chunks)
        shuf = runner.collect(runner.shuffle_chunks(chunks))
        ns, _ = runner.compare(real, shuf)
        results[name] = ns
        print(f"  {name} vs shuffled = {ns:3d} sig")

    return dict(results=results)

def direction_3(runner):
    """D3: Parameter sweep — Transition from spots to stripes (varying F)."""
    print("\n" + "=" * 60)
    print("D3: PARAMETER SWEEP (F: Spots -> Stripes)")
    print("=" * 60)

    # Regimes: spots(F=0.034, k=0.063) -> ... -> stripes(F=0.042, k=0.059)
    # Let's fix k=0.060 and vary F from 0.030 to 0.060
    # Expected: Chaos -> Spots -> Stripes -> Solitons? 
    # Actually, let's just sweep F across known regimes.
    
    k_fixed = 0.060
    f_values = [0.030, 0.035, 0.040, 0.045, 0.050, 0.055]
    
    baseline_F = f_values[0]
    baseline_chunks = [generate_gray_scott(rng, runner.data_size, F=baseline_F, k=k_fixed)
                       for rng in runner.trial_rngs()]
    baseline = runner.collect(baseline_chunks)

    results = {}
    for f_val in f_values:
        with runner.timed(f"F={f_val:.3f}"):
            chunks = [generate_gray_scott(rng, runner.data_size, F=f_val, k=k_fixed)
                      for rng in runner.trial_rngs(offset=int(f_val*1000))]
            met = runner.collect(chunks)
            ns, _ = runner.compare(baseline, met)
            results[f_val] = ns

    return dict(results=results, params=f_values)

def direction_4(runner):
    """D4: Robustness — Signal degradation with added observation noise."""
    print("\n" + "=" * 60)
    print("D4: ROBUSTNESS (Noise Tolerance)")
    print("=" * 60)

    # Use 'mazes' as the base pattern
    F, k = REGIMES['mazes']
    noise_levels = [0.0, 0.05, 0.10, 0.20, 0.40]
    
    base_chunks = [generate_gray_scott(rng, runner.data_size, F=F, k=k, noise_level=0.0)
                   for rng in runner.trial_rngs()]
    base_metrics = runner.collect(base_chunks)

    results = {}
    for nl in noise_levels:
        chunks = [generate_gray_scott(rng, runner.data_size, F=F, k=k, noise_level=nl)
                  for rng in runner.trial_rngs(offset=int(nl*100))]
        met = runner.collect(chunks)
        # Compare to clean baseline to see how much it deviates
        ns, _ = runner.compare(base_metrics, met)
        results[nl] = ns
        print(f"  Noise {nl:.2f}: {ns} metrics significantly different from clean")

    return dict(results=results, params=noise_levels)

def direction_5(runner):
    """D5: Scale test — Geometric stability across grid resolutions."""
    print("\n" + "=" * 60)
    print("D5: SCALE TEST (Grid Size)")
    print("=" * 60)

    sizes = [32, 48, 64, 80, 96]
    F, k = REGIMES['coral'] # Complex pattern
    
    # We compare each size to the previous size (interpolated? No, metrics are often scale invariant or not)
    # Actually, let's compare each size to a shuffled version of itself to see if detection power holds.
    # Or compare size N to size N (baseline is size N). That's 0.
    # Comparing different sizes is tricky because metrics might depend on size.
    # Let's measure "Structure Detection Power" vs Size. (Real vs Shuffled at each size).
    
    results = {}
    for sz in sizes:
        chunks = [generate_gray_scott(rng, sz, F=F, k=k)
                  for rng in runner.trial_rngs()]
        real = runner.collect(chunks)
        shuf = runner.collect(runner.shuffle_chunks(chunks))
        ns, _ = runner.compare(real, shuf)
        results[sz] = ns
        print(f"  Size {sz}x{sz}: {ns} metrics distinguish from shuffled")

    return dict(results=results, params=sizes)

# ==============================================================
# FIGURE
# ==============================================================
def make_figure(runner, d1, d2, d3, d4, d5):
    fig, axes = runner.create_figure(5, "Reaction-Diffusion Pattern Geometry")

    # D1: Heatmap
    runner.plot_heatmap(axes[0], d1['matrix'], d1['names'], "D1: Morphology Taxonomy")

    # D2: Bar chart
    names2 = list(d2['results'].keys())
    vals2 = [d2['results'][n] for n in names2]
    runner.plot_bars(axes[1], names2, vals2, "D2: Structure vs Shuffled")

    # D3: Line plot (Parameter Sweep)
    params3 = d3['params']
    vals3 = [d3['results'][p] for p in params3]
    runner.plot_line(axes[2], params3, vals3, "D3: F-Parameter Sweep (vs F=0.030)",
                     xlabel="Feed Rate F")

    # D4: Line plot (Noise)
    params4 = d4['params']
    vals4 = [d4['results'][p] for p in params4]
    runner.plot_line(axes[3], params4, vals4, "D4: Deviation vs Noise Level",
                     xlabel="Noise Std Dev")

    # D5: Line plot (Scale)
    params5 = d5['params']
    vals5 = [d5['results'][p] for p in params5]
    runner.plot_line(axes[4], params5, vals5, "D5: Detection Power vs Grid Size",
                     xlabel="Grid Size (NxN)", color="#FF9800")

    runner.save(fig, "reaction_diffusion_patterns")

# ==============================================================
# MAIN
# ==============================================================
def main():
    # Use 64x64 for speed in this demo investigation, but 128 is better.
    # Warning: 2000 steps at 128x128 x 25 trials x 6 regimes = SLOW.
    # We will use data_size=64 to keep it reasonable.
    runner = Runner("Reaction-Diffusion", mode="2d", data_size=64)

    print("=" * 60)
    print("REACTION-DIFFUSION GEOMETRY")
    print(f"Size={runner.data_size}x{runner.data_size}, Trials={runner.n_trials}")
    print("=" * 60)

    d1 = direction_1(runner)
    d2 = direction_2(runner)
    d3 = direction_3(runner)
    d4 = direction_4(runner)
    d5 = direction_5(runner)

    make_figure(runner, d1, d2, d3, d4, d5)

    pw = d1['matrix'][np.triu_indices_from(d1['matrix'], k=1)]
    runner.print_summary({
        'D1': f"Taxonomy: {pw.min():.0f}-{pw.max():.0f} sig (mean {pw.mean():.0f})",
        'D2': [f"{n} vs shuffled = {v}" for n, v in d2['results'].items()],
        'D3': "F sweep: " + ", ".join(
            f"{p}→{d3['results'][p]}" for p in d3['params']),
        'D4': "Noise: " + ", ".join(
            f"{p}→{d4['results'][p]}" for p in d4['params']),
        'D5': "Scale: " + ", ".join(
            f"{p}→{d5['results'][p]}" for p in d5['params']),
    })

if __name__ == "__main__":
    main()
